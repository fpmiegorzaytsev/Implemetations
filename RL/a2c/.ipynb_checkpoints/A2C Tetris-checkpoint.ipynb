{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a00da2-31e8-4c14-b2c4-cf73989b5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3, 4, 6'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.87'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e435de32-b6d7-4001-87ef-18aef1924b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, [cuda(id=0), cuda(id=1), cuda(id=2)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import matplotlib.pyplot as plt\n",
    "import flashbax as fbx\n",
    "import chex\n",
    "import warnings\n",
    "import jumanji\n",
    "from jumanji.wrappers import AutoResetWrapper\n",
    "import rlax\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "jax.device_count(), jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0965d72f-cf64-4ce5-a91f-a465695350df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import anakin_v3 as anakin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6893859-7805-4561-8ca3-3c67530b79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "LEARNING_RATE = 5e-4\n",
    "N_ENVS = 100\n",
    "GAMMA = 0.99\n",
    "N_ITERATIONS = 100\n",
    "COEFS = (1., 1., 0.01)\n",
    "\n",
    "TRAINING_EVAL_ITERS=100\n",
    "NUM_EVAL_EPISODES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f06c8e4-5677-496f-9a15-ed6f88b0189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jumanji.make(\"Tetris-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0b732e-0b6f-4540-99d5-1c411b54d23b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Observation' object has no attribute 'tetromino'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m n_devices, actor_fn, critic_fn, params, optim, opt_state, rng \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m params, opt_state, rngs_pv, rng \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbroadcast_to_pv_shape(n_devices, N_ENVS, params, opt_state, rng)\n\u001b[1;32m      4\u001b[0m learn_fn \u001b[38;5;241m=\u001b[39m anakin\u001b[38;5;241m.\u001b[39mget_learner_fn(\n\u001b[1;32m      5\u001b[0m     env,\n\u001b[1;32m      6\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mGAMMA,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     opt_update_fn\u001b[38;5;241m=\u001b[39moptim\u001b[38;5;241m.\u001b[39mupdate\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/a2c/utils.py:19\u001b[0m, in \u001b[0;36msetup_experiment\u001b[0;34m(env, seed, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m _, timestep \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(rng_reset)\n\u001b[1;32m     18\u001b[0m grid \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mexpand_dims(timestep\u001b[38;5;241m.\u001b[39mobservation\u001b[38;5;241m.\u001b[39mgrid, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m tetromino \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mexpand_dims(\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtetromino\u001b[49m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     20\u001b[0m n_actions_rotation, n_action_x_position \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_spec\u001b[38;5;241m.\u001b[39mnum_values\n\u001b[1;32m     22\u001b[0m network \u001b[38;5;241m=\u001b[39m ActorCritic(\u001b[38;5;28mint\u001b[39m(n_actions_rotation), \u001b[38;5;28mint\u001b[39m(n_action_x_position))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Observation' object has no attribute 'tetromino'"
     ]
    }
   ],
   "source": [
    "n_devices, actor_fn, critic_fn, params, optim, opt_state, rng = utils.setup_experiment(env, SEED, LEARNING_RATE)\n",
    "params, opt_state, rngs_pv, rng = utils.broadcast_to_pv_shape(n_devices, N_ENVS, params, opt_state, rng)\n",
    "\n",
    "learn_fn = anakin.get_learner_fn(\n",
    "    env,\n",
    "    gamma=GAMMA,\n",
    "    n_iterarions=N_ITERATIONS,\n",
    "    coefs=COEFS,\n",
    "    actor_fn=actor_fn,\n",
    "    critic_fn=critic_fn,\n",
    "    opt_update_fn=optim.update\n",
    ")\n",
    "\n",
    "learn_fn = jax.vmap(learn_fn, axis_name=\"envs\")\n",
    "learn_fn = jax.pmap(learn_fn, axis_name=\"devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4daa0d4c-812d-490e-aa49-57689005838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_one_episode(params, rng):\n",
    "  state, timestep = env.reset(rng)\n",
    "\n",
    "  def step_fn(val):\n",
    "\n",
    "    def sample_action(logits, action_mask, rng):\n",
    "        rng, rng_sample = jax.random.split(rng, 2)\n",
    "        masked_logits = utils.masked_fill(action_mask, logits, -jnp.inf)\n",
    "        num_columns = masked_logits.shape[1]\n",
    "        flattened_logits = masked_logits.reshape(-1, )\n",
    "        action = jnp.argmax(flattened_logits)\n",
    "        return jnp.stack([action // num_columns, action % num_columns])\n",
    "    \n",
    "    params, state, timestep, total_r, done, rng = val\n",
    "    rng, _ = jax.random.split(rng, 2)\n",
    "    \n",
    "    grid = timestep.observation.grid\n",
    "    tetromino = timestep.observation.tetromino\n",
    "    action_mask = timestep.observation.action_mask\n",
    "      \n",
    "    logits = actor_fn(\n",
    "        params,\n",
    "        jnp.expand_dims(timestep.observation.grid, (0, 3)),\n",
    "        jnp.expand_dims(timestep.observation.tetromino, (0, 3))\n",
    "      )\n",
    "    \n",
    "    action = sample_action(logits.squeeze(0), action_mask, rng)\n",
    "\n",
    "    next_state, next_timestep = env.step(state, action)\n",
    "    total_r += next_timestep.reward\n",
    "    return (params, next_state, next_timestep, total_r, next_timestep.last(), rng)\n",
    "\n",
    "  params, state, timestep, total_r, done, rng = jax.lax.while_loop(\n",
    "      lambda x: x[4] == False, step_fn, (params, state, timestep, 0, False, rng)\n",
    "  )\n",
    "  return params, total_r\n",
    "\n",
    "@jax.jit\n",
    "def eval(params, rng):\n",
    "  rngs = jax.random.split(rng, NUM_EVAL_EPISODES)\n",
    "  params = jax.tree.map(lambda x: x[0][0], params)\n",
    "  _, total_r = jax.lax.scan(eval_one_episode, params, rngs)\n",
    "  return jnp.mean(total_r), jnp.var(total_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f610bc-2dea-482c-91d3-ef213e1d7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = []\n",
    "for iter in tqdm(range(TRAINING_EVAL_ITERS)):\n",
    "    params, opt_state, rngs_pv = learn_fn(params, opt_state, rngs_pv)\n",
    "    \n",
    "    rng, rng_eval = jax.random.split(rng, 2)\n",
    "    total_r, total_r_var = eval(params, rng_eval)\n",
    "    avg_reward.append(total_r)\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(f\"Mean Reward at iteration {iter}: {total_r}\")\n",
    "    # print(f\"SE reward at iteration {iter}: {jnp.sqrt(total_r_var)}\")\n",
    "    \n",
    "    plt.plot(avg_reward)\n",
    "    # plt.plot(jnp.array(avg_reward) + jnp.sqrt(jnp.array(bounds_avg_reward)), linestyle='dashed', color='red')\n",
    "    # plt.plot(jnp.array(avg_reward) - jnp.sqrt(jnp.array(bounds_avg_reward)), linestyle='dashed', color='red')\n",
    "    plt.title(\"Average reward each iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52b1d4-a55f-4804-9f02-4e3dfc3759ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# states = []\n",
    "# rng, rng_gif = jax.random.split(rng, 2)\n",
    "# state, timestep = env.reset(rng_gif)\n",
    "# params = jax.tree.map(lambda x: jnp.mean(x, axis=(0, 1)), params_state.online)\n",
    "\n",
    "# for i in tqdm(range(500)):\n",
    "#     states.append(state)\n",
    "#     obs = timestep.observation.board\n",
    "#     action_mask = timestep.observation.action_mask\n",
    "#     q_values = network.apply(params, jnp.expand_dims(obs, (0, 3))).squeeze(0)\n",
    "#     q_values = utils.masked_fill(action_mask, q_values, -jnp.inf)\n",
    "#     action = jnp.argmax(q_values)\n",
    "#     state, timestep = env.step(state, action)\n",
    "#     if timestep.last():\n",
    "#         break\n",
    "\n",
    "# env.animate(states, interval=150).save(\"./2048.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd8b81-3916-4426-9db3-1ea6e5d1cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_count = sum(x.size for x in jax.tree.leaves(params))\n",
    "param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9200e0d5-f462-495b-bcea-dfb32a26559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "24011736 // 10 ** 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b57830-5af1-44a0-898c-26809860bc96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
